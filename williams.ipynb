{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "572fb63a",
   "metadata": {},
   "source": [
    "# Williams-Style Space–Time Tradeoff: Bench Notebook\n",
    "Benchmarks your local `enhanced_transformer.py` module using three policies:\n",
    "\n",
    "- **cache** (standard KV cache)\n",
    "- **grouped** KV cache with fewer KV heads (saves cache memory)\n",
    "- **recompute** tiled attention (Williams tradeoff: recompute instead of storing, bounded by Qc×Kc)\n",
    "\n",
    "It records **runtime**, **peak GPU memory** (if CUDA), and **ΔRSS** for CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189e31c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "import bench_williams as bw\n",
    "import enhanced_transformer as et\n",
    "bw, et = reload(bw), reload(et)\n",
    "print('Device:', 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('TransformerBlock exists:', hasattr(et, 'TransformerBlock'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3874bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_list = [1024, 4096, 8192, 16384]\n",
    "configs = [\n",
    "    {\"name\":\"cache\", \"policy\":\"cache\"},\n",
    "    {\"name\":\"grouped(kv=2)\", \"policy\":\"grouped\", \"kv_heads\":2},\n",
    "    {\"name\":\"recompute(Q64,K256)\", \"policy\":\"recompute\", \"q_chunk\":64, \"k_chunk\":256},\n",
    "]\n",
    "rows = bw.bench_grid(seq_list, configs, runs=3, d_model=256, heads=8, device=None, warmup=1)\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54cc405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from caas_jupyter_tools import display_dataframe_to_user\n",
    "display_dataframe_to_user('Williams Bench Results', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171a481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime vs sequence length\n",
    "plt.figure()\n",
    "for name in df['name'].unique():\n",
    "    sub = df[df['name'] == name].sort_values('seq')\n",
    "    plt.plot(sub['seq'], sub['time_mean_s'], marker='o', label=name)\n",
    "plt.xlabel('Sequence length')\n",
    "plt.ylabel('Mean runtime (s)')\n",
    "plt.title('Runtime vs Sequence Length')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c2408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peak GPU memory vs sequence length (if CUDA)\n",
    "if torch.cuda.is_available():\n",
    "    plt.figure()\n",
    "    for name in df['name'].unique():\n",
    "        sub = df[df['name'] == name].sort_values('seq')\n",
    "        plt.plot(sub['seq'], sub['gpu_peak_MB'], marker='o', label=name)\n",
    "    plt.xlabel('Sequence length')\n",
    "    plt.ylabel('Peak GPU memory (MB)')\n",
    "    plt.title('Peak GPU Memory vs Sequence Length')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('CUDA not available: skipping GPU memory plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5190aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process RSS delta (MB) vs sequence length\n",
    "plt.figure()\n",
    "for name in df['name'].unique():\n",
    "    sub = df[df['name'] == name].sort_values('seq')\n",
    "    plt.plot(sub['seq'], sub['rss_delta_MB'], marker='o', label=name)\n",
    "plt.xlabel('Sequence length')\n",
    "plt.ylabel('ΔRSS (MB)')\n",
    "plt.title('Process RSS Delta vs Sequence Length')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

